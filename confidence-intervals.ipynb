{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.1 [1-sided bounds] Check whether they correctly implement the mathematics. Correct them if not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.2 [1-sided bounds] Check the endpoints are found in a numerically stable and efficient manner. Provide a better method if not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.1 [2-sided bounds] Check whether they correctly implement the mathematics. Correct them if not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As far as I can tell, the mathematics are implemented correctly. Both of the current implementations of the two-sided binomial and hypergeometric confidence intervals are based on the Clopper-Pearson computation method. This is computed by dividing alpha by two and computing two separate confidence intervals based on the new alpha, and using the overlap as the confidence interval. The existing code is correct and optimizes for q (the total number of good objects in the population in `cl - hypergeom.cdf(x - 1, N, q, n)` for lower confidence interval, and `hypergeom.cdf(x, N, q, n) - (1 - cl)` for upper confidence interval. For binomial, q is optimized (population probability) in `cl - binom.cdf(x - 1, n, q)` for lower confidence intervals and `binom.cdf(x, n, q) - (1 - cl)` for upper confidence intervals. These are used to find the lower and upper bounds, and then combined to form 2-sided confidence intervals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.2 [2-sided bounds] Check the endpoints are found in a numerically stable and efficient manner. Provide a better method if not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The current implementation of both the hypergeometric and binomial 2-sided bounds confidence intervals use a root finding optimization method called brentq. According to SciPy, \"Brent (1973) claims convergence is guaranteed for functions computable within [a,b].\" One of the limitations with Brentq is that it may occasionally return 0.0 when (a) in [a,b] is smaller than 1e^-3, making the return be outside the interval. Two similarly computationally expensive optimization techniques are brenth and ridder.\n",
    "\n",
    "The existing implementation for both types of 2-sided confidence intervals include a while loop to ensure that f(a) and f(b) are of opposite signs before passing it into the optimization function to make sure the code doesn't break. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.4 Calculate (not simulate) the expected width of the 2-sided 95% confidence intervals for method=\"clopper-pearson\" , method=\"sterne\", and method=\"wang\" for a range of values of ùëõ and ùëù (for the binomial using Clopper-Pearson and Sterne) and for ùëÅ, ùê∫, and ùëõ (for the hypergeometric using Clopper-Pearson, Sterne, and Wang)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Method = Clopper-Pearson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Method = Wang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Method = Sterne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discuss the differences among the three methods. You might consider how long it takes the methods to run and the expected lengths of the intervals as the parameters vary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clopper-Pearson ensures that the tails are of equal size. In Sterne's tails, they may be of different sizes, and create a potentially tighter confidence interval than Clopper-Pearson.\n",
    "\n",
    "THIS NEEDS MORE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Which would you recommend, and why? If you would recommend one method over the other in some circumstances but not in others, explain why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
